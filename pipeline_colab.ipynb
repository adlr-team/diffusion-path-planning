{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRAev-KUifbW",
        "outputId": "f70d8a10-4ddb-4bd6-8f38-cbc841993bb5"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APRB2VcQU5C2"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from diffuser.utils.config import Config\n",
        "import os\n",
        "import collections\n",
        "import numpy as np\n",
        "import pdb\n",
        "from minari import DataCollector, StepDataCallback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88Fd7E-bLXNI"
      },
      "source": [
        "## Diffuser Lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rg9GmmXWStiK"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "    def __init__(\n",
        "        self,\n",
        "        loader,\n",
        "        savepath,\n",
        "        dataset,\n",
        "        horizon,\n",
        "        normalizer,\n",
        "        preprocess_fns,\n",
        "        use_padding,\n",
        "        max_path_length,\n",
        "        renderer,\n",
        "        model,\n",
        "        dim_mults,\n",
        "        device,\n",
        "    ):\n",
        "        self.loader = loader\n",
        "        self.savepath = savepath\n",
        "        self.dataset = dataset\n",
        "        self.horizon = horizon\n",
        "        self.normalizer = normalizer\n",
        "        self.preprocess_fns = preprocess_fns\n",
        "        self.use_padding = use_padding\n",
        "        self.max_path_length = max_path_length\n",
        "        self.renderer = renderer\n",
        "        self.model = model\n",
        "        # self.transition_dim=transition_dim\n",
        "        # self.cond_dim=cond_dim\n",
        "        self.dim_mults = dim_mults\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "args = Args(\n",
        "    loader=\"datasets.sequence.GoalDataset\",\n",
        "    savepath=\"saved/\",\n",
        "    dataset=\"PointMaze_Large-v3\",\n",
        "    horizon=256,\n",
        "    normalizer=\"LimitsNormalizer\",\n",
        "    preprocess_fns=[\"maze2d_set_terminals\"],\n",
        "    use_padding=False,\n",
        "    max_path_length=40000,\n",
        "    renderer=\"utils.rendering.Maze2dRenderer\",\n",
        "    model=\"models.temporal.TemporalUnet\",\n",
        "    dim_mults=(1, 4, 8),\n",
        "    device=\"cpu\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzUNVoCyS54w",
        "outputId": "8fd0ca8e-cce7-456b-9438-15ca2be74525"
      },
      "outputs": [],
      "source": [
        "dataset_config = Config(\n",
        "    args.loader,\n",
        "    savepath=(args.savepath, \"dataset_config.pkl\"),\n",
        "    env=args.dataset,\n",
        "    horizon=args.horizon,\n",
        "    normalizer=args.normalizer,\n",
        "    preprocess_fns=args.preprocess_fns,\n",
        "    use_padding=args.use_padding,\n",
        "    max_path_length=args.max_path_length,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yN1wOeVGTaft",
        "outputId": "1d0e9bfe-1bca-4718-a6e9-03954dd5dbc9"
      },
      "outputs": [],
      "source": [
        "dataset = dataset_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvk2JJzc6BO6",
        "outputId": "89b725cd-4487-4716-e55f-9b346b7e4e95"
      },
      "outputs": [],
      "source": [
        "# dataset[1]\n",
        "# dataset[1].observations[\"observation\"][:, :2]\n",
        "# dataset[1].total_steps\n",
        "# env = gym.make(\"PointMaze_Large-v3\")\n",
        "# env._max_episode_steps\n",
        "# # env.target\n",
        "# np.array(wrapped_env.observation_space[\"desired_goal\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENDornlpjRvY"
      },
      "source": [
        "## Configs for Rendering and the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bme6emjGVS6-",
        "outputId": "847b6b49-3758-4e5a-9e0c-3e3872f65259"
      },
      "outputs": [],
      "source": [
        "render_config = Config(\n",
        "    args.renderer,\n",
        "    savepath=(args.savepath, \"render_config.pkl\"),\n",
        "    env=args.dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtDYVRdctm86",
        "outputId": "3c4a10d7-ccde-41d4-f7cf-19404180f063"
      },
      "outputs": [],
      "source": [
        "# observation_dim = dataset.observation_dim\n",
        "# action_dim = dataset.action_dim\n",
        "observation_dim = 4\n",
        "action_dim = 2\n",
        "# -----------------------------------------------------------------------------#\n",
        "# ------------------------------ model & trainer ------------------------------#\n",
        "# -----------------------------------------------------------------------------#\n",
        "\n",
        "model_config = Config(\n",
        "    args.model,\n",
        "    savepath=(args.savepath, \"model_config.pkl\"),\n",
        "    horizon=args.horizon,\n",
        "    transition_dim=observation_dim + action_dim,\n",
        "    cond_dim=observation_dim,\n",
        "    dim_mults=args.dim_mults,\n",
        "    device=args.device,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLILqE_ZrBdy",
        "outputId": "2a032e3e-bf83-48a1-f186-e1c5a62a5e3c"
      },
      "outputs": [],
      "source": [
        "renderer = render_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0362NXnEtqzJ",
        "outputId": "b3be0703-1e9a-4fb4-fba6-bc14a3052215"
      },
      "outputs": [],
      "source": [
        "model = model_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56UTrirwtof5",
        "outputId": "a0d92bcc-c4f6-41bd-fa4f-54d7887afa4a"
      },
      "outputs": [],
      "source": [
        "diffusion_config = Config(\n",
        "    _class=\"models.diffuser.GaussianDiffusion\",\n",
        "    savepath=(args.savepath, \"diffusion_config.pkl\"),\n",
        "    horizon=256,\n",
        "    observation_dim=observation_dim,\n",
        "    action_dim=2,\n",
        "    n_timesteps=256,\n",
        "    loss_type=\"l2\",\n",
        "    clip_denoised=True,\n",
        "    predict_epsilon=False,\n",
        "    # loss weighting\n",
        "    action_weight=1,\n",
        "    loss_weights=None,\n",
        "    loss_discount=1,\n",
        "    device=args.device,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUBUMLX-KX-D"
      },
      "outputs": [],
      "source": [
        "diffuser = diffusion_config(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcJZFQl9jXtn"
      },
      "source": [
        "## Forward pass is working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SnQpWsjKlRQ",
        "outputId": "6c4ce3f1-c507-42e2-e51b-485e83ed51ac"
      },
      "outputs": [],
      "source": [
        "from diffuser.utils.arrays import report_parameters, batchify\n",
        "\n",
        "report_parameters(model)\n",
        "\n",
        "print(\"Testing forward...\", end=\" \", flush=True)\n",
        "batch = batchify(dataset[0])\n",
        "loss, _ = diffuser.loss(*batch)\n",
        "loss.backward()\n",
        "print(\"âœ“\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2zObsfx8uFq"
      },
      "source": [
        "## Using the trainer requires taking care of the 'device' in the folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4Ahsw3rhEeF",
        "outputId": "72015822-2ea3-4d8d-df37-76026af74d80"
      },
      "outputs": [],
      "source": [
        "from diffuser.utils.training import Trainer\n",
        "\n",
        "trainer_config = Config(\n",
        "    Trainer,\n",
        "    savepath=(args.savepath, \"trainer_config.pkl\"),\n",
        "    train_batch_size=32,\n",
        "    train_lr=2e-4,\n",
        "    gradient_accumulate_every=2,\n",
        "    ema_decay=0.005,\n",
        "    sample_freq=1000,\n",
        "    save_freq=1000,\n",
        "    label_freq=int(2e4 // 50),\n",
        "    save_parallel=False,\n",
        "    results_folder=args.savepath,\n",
        "    bucket=None,\n",
        "    n_reference=1,\n",
        "    n_samples=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcB-OnzvkFgn"
      },
      "outputs": [],
      "source": [
        "trainer = trainer_config(diffuser, dataset, renderer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRJlcy_WKXdq"
      },
      "source": [
        "# Training process inlcluding rendering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "id": "IaDt0qR3kczo",
        "outputId": "1ac1ee61-695b-4017-cd62-41bf7449b7b9"
      },
      "outputs": [],
      "source": [
        "# n_epochs = int(args.n_train_steps // args.n_steps_per_epoch)\n",
        "n_epochs = 2\n",
        "for i in range(n_epochs):\n",
        "    print(f\"Epoch {i} / {n_epochs} | {args.savepath}\")\n",
        "    trainer.train(n_train_steps=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh7nFzc5jxhB"
      },
      "source": [
        "## Random Test (don't run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDUP3qKtE5Kw",
        "outputId": "518943ce-53b7-483f-c3bf-b73305daee0b"
      },
      "outputs": [],
      "source": [
        "from contextlib import (\n",
        "    contextmanager,\n",
        "    redirect_stderr,\n",
        "    redirect_stdout,\n",
        ")\n",
        "\n",
        "\n",
        "@contextmanager\n",
        "def suppress_output():\n",
        "    \"\"\"\n",
        "    A context manager that redirects stdout and stderr to devnull\n",
        "    https://stackoverflow.com/a/52442331\n",
        "    \"\"\"\n",
        "    with open(os.devnull, \"w\") as fnull:\n",
        "        with redirect_stderr(fnull) as err, redirect_stdout(fnull) as out:\n",
        "            yield (err, out)\n",
        "\n",
        "\n",
        "# with suppress_output():\n",
        "#     ## d4rl prints out a variety of warnings\n",
        "#     import d4rl\n",
        "\n",
        "# -----------------------------------------------------------------------------#\n",
        "# -------------------------------- general api --------------------------------#\n",
        "# -----------------------------------------------------------------------------#\n",
        "\n",
        "\n",
        "def load_environment(name):\n",
        "    print(name)\n",
        "    if type(name) != str:\n",
        "        # name is already an environment\n",
        "        return name\n",
        "    with suppress_output():\n",
        "        wrapped_env = gym.make(name)\n",
        "    env = wrapped_env.unwrapped\n",
        "    env.max_episode_steps = wrapped_env._max_episode_steps\n",
        "    env.name = name\n",
        "    return env\n",
        "\n",
        "\n",
        "class PointMazeStepDataCallback(StepDataCallback):\n",
        "    \"\"\"Add environment state information to 'infos'.\n",
        "\n",
        "    Also, since the environment generates a new target every time it reaches a goal, the environment is\n",
        "    never terminated or truncated. This callback overrides the truncation value to True when the step\n",
        "    returns a True 'succes' key in 'infos'. This way we can divide the Minari dataset into different trajectories.\n",
        "    \"\"\"\n",
        "\n",
        "    def __call__(\n",
        "        self, env, obs, info, action=None, rew=None, terminated=None, truncated=None\n",
        "    ):\n",
        "        qpos = obs[\"observation\"][:2]\n",
        "        qvel = obs[\"observation\"][2:]\n",
        "        goal = obs[\"desired_goal\"]\n",
        "\n",
        "        step_data = super().__call__(env, obs, info, action, rew, terminated, truncated)\n",
        "\n",
        "        if step_data[\"infos\"][\"success\"]:\n",
        "            step_data[\"truncation\"] = True\n",
        "        step_data[\"infos\"][\"qpos\"] = qpos\n",
        "        step_data[\"infos\"][\"qvel\"] = qvel\n",
        "        step_data[\"infos\"][\"goal\"] = goal\n",
        "\n",
        "        return step_data\n",
        "\n",
        "\n",
        "UP = 0\n",
        "DOWN = 1\n",
        "LEFT = 2\n",
        "RIGHT = 3\n",
        "\n",
        "EXPLORATION_ACTIONS = {UP: (0, 1), DOWN: (0, -1), LEFT: (-1, 0), RIGHT: (1, 0)}\n",
        "\n",
        "\n",
        "class QIteration:\n",
        "    \"\"\"Solves for optimal policy with Q-Value Iteration.\n",
        "\n",
        "    Inspired by https://github.com/Farama-Foundation/D4RL/blob/master/d4rl/pointmaze/q_iteration.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, maze):\n",
        "        self.maze = maze\n",
        "        self.num_states = maze.map_length * maze.map_width\n",
        "        self.num_actions = len(EXPLORATION_ACTIONS.keys())\n",
        "        self.rew_matrix = np.zeros((self.num_states, self.num_actions))\n",
        "        self.compute_transition_matrix()\n",
        "\n",
        "    def generate_path(self, current_cell, goal_cell):\n",
        "        self.compute_reward_matrix(goal_cell)\n",
        "        q_values = self.get_q_values()\n",
        "        current_state = self.cell_to_state(current_cell)\n",
        "        waypoints = {}\n",
        "        while True:\n",
        "            action_id = np.argmax(q_values[current_state])\n",
        "            next_state, _ = self.get_next_state(\n",
        "                current_state, EXPLORATION_ACTIONS[action_id]\n",
        "            )\n",
        "            current_cell = self.state_to_cell(current_state)\n",
        "            waypoints[current_cell] = self.state_to_cell(next_state)\n",
        "            if waypoints[current_cell] == goal_cell:\n",
        "                break\n",
        "\n",
        "            current_state = next_state\n",
        "\n",
        "        return waypoints\n",
        "\n",
        "    def reward_function(self, desired_cell, current_cell):\n",
        "        if desired_cell == current_cell:\n",
        "            return 1.0\n",
        "        else:\n",
        "            return 0.0\n",
        "\n",
        "    def state_to_cell(self, state):\n",
        "        i = int(state / self.maze.map_width)\n",
        "        j = state % self.maze.map_width\n",
        "        return (i, j)\n",
        "\n",
        "    def cell_to_state(self, cell):\n",
        "        return cell[0] * self.maze.map_width + cell[1]\n",
        "\n",
        "    def get_q_values(self, num_itrs=50, discount=0.99):\n",
        "        q_fn = np.zeros((self.num_states, self.num_actions))\n",
        "        for _ in range(num_itrs):\n",
        "            v_fn = np.max(q_fn, axis=1)\n",
        "            q_fn = self.rew_matrix + discount * self.transition_matrix.dot(v_fn)\n",
        "        return q_fn\n",
        "\n",
        "    def compute_reward_matrix(self, goal_cell):\n",
        "        for state in range(self.num_states):\n",
        "            for action in range(self.num_actions):\n",
        "                next_state, _ = self.get_next_state(state, EXPLORATION_ACTIONS[action])\n",
        "                next_cell = self.state_to_cell(next_state)\n",
        "                self.rew_matrix[state, action] = self.reward_function(\n",
        "                    goal_cell, next_cell\n",
        "                )\n",
        "\n",
        "    def compute_transition_matrix(self):\n",
        "        \"\"\"Constructs this environment's transition matrix.\n",
        "        Returns:\n",
        "          A dS x dA x dS array where the entry transition_matrix[s, a, ns]\n",
        "          corresponds to the probability of transitioning into state ns after taking\n",
        "          action a from state s.\n",
        "        \"\"\"\n",
        "        self.transition_matrix = np.zeros(\n",
        "            (self.num_states, self.num_actions, self.num_states)\n",
        "        )\n",
        "        for state in range(self.num_states):\n",
        "            for action_idx, action in EXPLORATION_ACTIONS.items():\n",
        "                next_state, valid = self.get_next_state(state, action)\n",
        "                if valid:\n",
        "                    self.transition_matrix[state, action_idx, next_state] = 1\n",
        "\n",
        "    def get_next_state(self, state, action):\n",
        "        cell = self.state_to_cell(state)\n",
        "\n",
        "        next_cell = tuple(map(lambda i, j: int(i + j), cell, action))\n",
        "        next_state = self.cell_to_state(next_cell)\n",
        "\n",
        "        return next_state, self._check_valid_cell(next_cell)\n",
        "\n",
        "    def _check_valid_cell(self, cell):\n",
        "        # Out of map bounds\n",
        "        if cell[0] >= self.maze.map_length:\n",
        "            return False\n",
        "        elif cell[1] >= self.maze.map_width:\n",
        "            return False\n",
        "        # Wall collision\n",
        "        elif self.maze.maze_map[cell[0]][cell[1]] == 1:\n",
        "            return False\n",
        "        else:\n",
        "            return True\n",
        "\n",
        "\n",
        "class WaypointController:\n",
        "    \"\"\"Agent controller to follow waypoints in the maze.\n",
        "\n",
        "    Inspired by https://github.com/Farama-Foundation/D4RL/blob/master/d4rl/pointmaze/waypoint_controller.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, maze, gains={\"p\": 10.0, \"d\": -1.0}, waypoint_threshold=0.1):\n",
        "        self.global_target_xy = np.empty(2)\n",
        "        self.maze = maze\n",
        "\n",
        "        self.maze_solver = QIteration(maze=self.maze)\n",
        "\n",
        "        self.gains = gains\n",
        "        self.waypoint_threshold = waypoint_threshold\n",
        "        self.waypoint_targets = None\n",
        "\n",
        "    def compute_action(self, obs):\n",
        "        # Check if we need to generate new waypoint path due to change in global target\n",
        "        if (\n",
        "            np.linalg.norm(self.global_target_xy - obs[\"desired_goal\"]) > 1e-3\n",
        "            or self.waypoint_targets is None\n",
        "        ):\n",
        "            # Convert xy to cell id\n",
        "            achieved_goal_cell = tuple(\n",
        "                self.maze.cell_xy_to_rowcol(obs[\"achieved_goal\"])\n",
        "            )\n",
        "            self.global_target_id = tuple(\n",
        "                self.maze.cell_xy_to_rowcol(obs[\"desired_goal\"])\n",
        "            )\n",
        "            self.global_target_xy = obs[\"desired_goal\"]\n",
        "\n",
        "            self.waypoint_targets = self.maze_solver.generate_path(\n",
        "                achieved_goal_cell, self.global_target_id\n",
        "            )\n",
        "\n",
        "            # Check if the waypoint dictionary is empty\n",
        "            # If empty then the ball is already in the target cell location\n",
        "            if self.waypoint_targets:\n",
        "                self.current_control_target_id = self.waypoint_targets[\n",
        "                    achieved_goal_cell\n",
        "                ]\n",
        "                self.current_control_target_xy = self.maze.cell_rowcol_to_xy(\n",
        "                    np.array(self.current_control_target_id)\n",
        "                )\n",
        "            else:\n",
        "                self.waypoint_targets[self.current_control_target_id] = (\n",
        "                    self.current_control_target_id\n",
        "                )\n",
        "                self.current_control_target_id = self.global_target_id\n",
        "                self.current_control_target_xy = self.global_target_xy\n",
        "\n",
        "        # Check if we need to go to the next waypoint\n",
        "        dist = np.linalg.norm(self.current_control_target_xy - obs[\"achieved_goal\"])\n",
        "        if (\n",
        "            dist <= self.waypoint_threshold\n",
        "            and self.current_control_target_id != self.global_target_id\n",
        "        ):\n",
        "            self.current_control_target_id = self.waypoint_targets[\n",
        "                self.current_control_target_id\n",
        "            ]\n",
        "            # If target is global goal go directly to goal position\n",
        "            if self.current_control_target_id == self.global_target_id:\n",
        "                self.current_control_target_xy = self.global_target_xy\n",
        "            else:\n",
        "                self.current_control_target_xy = (\n",
        "                    self.maze.cell_rowcol_to_xy(\n",
        "                        np.array(self.current_control_target_id)\n",
        "                    )\n",
        "                    - np.random.uniform(size=(2,)) * 0.2\n",
        "                )\n",
        "\n",
        "        action = (\n",
        "            self.gains[\"p\"] * (self.current_control_target_xy - obs[\"achieved_goal\"])\n",
        "            + self.gains[\"d\"] * obs[\"observation\"][2:]\n",
        "        )\n",
        "        action = np.clip(action, -1, 1)\n",
        "\n",
        "        return action\n",
        "\n",
        "\n",
        "def get_dataset():\n",
        "    # Code from Minari documentation\n",
        "    dataset_name = \"pointmaze-umaze-v0\"\n",
        "    total_steps = 20_000\n",
        "\n",
        "    # continuing task => the episode doesn't terminate or truncate when reaching a goal\n",
        "    # it will generate a new target. For this reason we set the maximum episode steps to\n",
        "    # the desired size of our Minari dataset (evade truncation due to time limit)\n",
        "    env = gym.make(\n",
        "        \"PointMaze_Medium-v3\", continuing_task=True, max_episode_steps=total_steps // 2\n",
        "    )\n",
        "\n",
        "    # Data collector wrapper to save temporary data while stepping. Characteristics:\n",
        "    #   * Custom StepDataCallback to add extra state information to 'infos' and divide dataset in different episodes by overridng\n",
        "    #     truncation value to True when target is reached\n",
        "    #   * Record the 'info' value of every step\n",
        "    collector_env = DataCollector(\n",
        "        env, step_data_callback=PointMazeStepDataCallback, record_infos=True\n",
        "    )\n",
        "\n",
        "    obs, _ = collector_env.reset(seed=123)\n",
        "\n",
        "    waypoint_controller = WaypointController(maze=env.maze)\n",
        "\n",
        "    for n_step in range(int(total_steps)):\n",
        "        action = waypoint_controller.compute_action(obs)\n",
        "        # Add some noise to each step action\n",
        "        action += np.random.randn(*action.shape) * 0.5\n",
        "        action = np.clip(\n",
        "            action, env.action_space.low, env.action_space.high, dtype=np.float32\n",
        "        )\n",
        "\n",
        "        obs, rew, terminated, truncated, info = collector_env.step(action)\n",
        "\n",
        "    dataset = collector_env.create_dataset(\n",
        "        dataset_id=dataset_name,\n",
        "        algorithm_name=\"QIteration\",\n",
        "        code_permalink=\"https://github.com/Farama-Foundation/Minari/blob/main/docs/tutorials/dataset_creation/point_maze_dataset.py\",\n",
        "        author=\"Rodrigo Perez-Vicente\",\n",
        "        author_email=\"rperezvicente@farama.org\",\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "\n",
        "dataset = get_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "Qb0DuGNZQMbE",
        "outputId": "2dbcd2a9-9641-4daa-92bb-956bc8a03754"
      },
      "outputs": [],
      "source": [
        "print(dataset)\n",
        "print(dataset[\"actions\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "vugWLtYdZedT",
        "outputId": "12030896-dac2-4e04-f7c9-751cf3f9d8b1"
      },
      "outputs": [],
      "source": [
        "reshaped = {}\n",
        "for key, val in dataset.items():\n",
        "    dim = val.shape[-1]\n",
        "    reshaped[key] = val.reshape(-1, dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVkGiVc-ZFf_"
      },
      "outputs": [],
      "source": [
        "def process_maze2d_episode(episode):\n",
        "    '''\n",
        "        adds in `next_observations` field to episode\n",
        "    '''\n",
        "    #assert 'next_observations' not in episode\n",
        "    ep = {}\n",
        "    length = len(episode.observations)\n",
        "    next_observations = episode.observations[\"observation\"][1:].copy()\n",
        "    for key, val in episode.observations.items():\n",
        "        ep[key] = val[:-1]\n",
        "    ep['next_observations'] = next_observations\n",
        "    for attr, value in vars(episode).items():\n",
        "      if attr == \"observations\":\n",
        "        continue\n",
        "      elif attr == \"terminations\":\n",
        "        ep[\"terminals\"] = value\n",
        "      elif attr == \"truncations\":\n",
        "        ep[\"timeouts\"] = value\n",
        "      elif attr == \"id\" or attr == \"seed\" or attr == \"total_timesteps\" or \"infos\"\n",
        "        continue\n",
        "      else:\n",
        "        ep[attr] = value\n",
        "\n",
        "    return ep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkYksqT8aY8d"
      },
      "outputs": [],
      "source": [
        "a = process_maze2d_episode(dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcVb08Q4PTPG",
        "outputId": "c936ba38-1dc3-46b8-ebff-71d2ba710099"
      },
      "outputs": [],
      "source": [
        "print(a.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nxO9NIoTfrH",
        "outputId": "4b5b3977-db46-4e29-b86a-38cf6a6531b9"
      },
      "outputs": [],
      "source": [
        "a[\"terminals\"].any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvKzMJmf5nSP",
        "outputId": "a8fb30b5-c704-465d-e83a-2c02f22db903"
      },
      "outputs": [],
      "source": [
        "dataset[0].infos[\"goal\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8udmHX4-QPDR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JUQtkJu97Np",
        "outputId": "1cc825f3-90ab-4968-8ec4-dc08c2e94674"
      },
      "outputs": [],
      "source": [
        "for attr, value in vars(dataset[0]).items():\n",
        "    if attr == \"observations\":\n",
        "        continue\n",
        "    print(f\"{attr}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vatRKfTeUzKc",
        "outputId": "94502865-a78e-4180-dd1d-b2dd1d254fa7"
      },
      "outputs": [],
      "source": [
        "# dataset = get_dataset()\n",
        "# dataset = preprocess_fn(dataset)\n",
        "\n",
        "# N = dataset['rewards'].shape[0]\n",
        "N = dataset[0].rewards.shape[0]\n",
        "# N = dataset._data.total_episodes\n",
        "data_ = collections.defaultdict(list)\n",
        "\n",
        "# The newer version of the dataset adds an explicit\n",
        "# timeouts field. Keep old method for backwards compatability.\n",
        "use_timeouts = False\n",
        "print(N)\n",
        "episode_step = 0\n",
        "for i in range(N):\n",
        "    # print(dataset[i])\n",
        "    # dataset[i].observations\n",
        "    # done_bool = bool(dataset['terminals'][i])\n",
        "    done_bool = bool(dataset[episode_step].terminations[i])\n",
        "\n",
        "    final_timestep = episode_step == env._max_episode_steps - 1\n",
        "\n",
        "    # for k in dataset:\n",
        "    for attr, value in vars(dataset[episode_step]).items():\n",
        "        # if 'metadata' in k: continue\n",
        "        # data_[k].append(dataset[k][i])\n",
        "        data_[attr].append(value)\n",
        "\n",
        "    if done_bool or final_timestep:\n",
        "        episode_step = 0\n",
        "        episode_data = {}\n",
        "        for k in data_:\n",
        "            episode_data[k] = np.array(data_[k])\n",
        "        episode_data = process_maze2d_episode(dataset[i])\n",
        "        print(episode_data)\n",
        "        data_ = collections.defaultdict(list)\n",
        "\n",
        "        episode_step += 1\n",
        "    break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "yP4EZWUtjLEW",
        "Xh7nFzc5jxhB"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
