{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRAev-KUifbW",
        "outputId": "f70d8a10-4ddb-4bd6-8f38-cbc841993bb5"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "APRB2VcQU5C2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/magic-rabbit/tum-adlr-ss24-18/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "from diffuser.utils.config import Config, get_params, get_device_settings\n",
        "from diffuser.utils.training import Trainer\n",
        "from diffuser.utils.arrays import report_parameters, batchify\n",
        "\n",
        "import os\n",
        "import collections\n",
        "import numpy as np\n",
        "import pdb\n",
        "from minari import DataCollector, StepDataCallback\n",
        "import torch\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "from datetime import datetime\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88Fd7E-bLXNI"
      },
      "source": [
        "## Parse Arguments and Paramters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The used device is gpu!\n"
          ]
        }
      ],
      "source": [
        "# Get settings from the config file\n",
        "\n",
        "parser = get_params()\n",
        "\n",
        "args = parser.parse_known_args(sys.argv[1:])[0]\n",
        "\n",
        "# Set Seeds\n",
        "seed = args.seed\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Get device settings\n",
        "device = get_device_settings(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzUNVoCyS54w",
        "outputId": "8fd0ca8e-cce7-456b-9438-15ca2be74525"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_class:datasets.sequence.GoalDataset\n",
            "[ utils/config ] Imported diffuser.datasets.sequence:GoalDataset\n",
            "\n",
            "[utils/config ] Config: <class 'diffuser.datasets.sequence.GoalDataset'>\n",
            "    env: PointMaze_Medium-v3\n",
            "    horizon: 256\n",
            "    max_path_length: 10000\n",
            "    normalizer: LimitsNormalizer\n",
            "    preprocess_fns: ['maze2d_set_terminals']\n",
            "    use_padding: False\n",
            "\n",
            "[ utils/config ] Saved config to: saved/dataset_config.pkl\n",
            "\n",
            "_class:utils.rendering.Maze2dRenderer\n",
            "[ utils/config ] Imported diffuser.utils.rendering:Maze2dRenderer\n",
            "\n",
            "[utils/config ] Config: <class 'diffuser.utils.rendering.Maze2dRenderer'>\n",
            "    env: PointMaze_Medium-v3\n",
            "\n",
            "[ utils/config ] Saved config to: saved/render_config.pkl\n",
            "\n",
            "_class:models.temporal.TemporalUnet\n",
            "[ utils/config ] Imported diffuser.models.temporal:TemporalUnet\n",
            "\n",
            "[utils/config ] Config: <class 'diffuser.models.temporal.TemporalUnet'>\n",
            "    cond_dim: 4\n",
            "    dim_mults: (1, 4, 8)\n",
            "    horizon: 256\n",
            "    transition_dim: 6\n",
            "\n",
            "[ utils/config ] Saved config to: saved/model_config.pkl\n",
            "\n",
            "_class:models.diffuser.GaussianDiffusion\n",
            "[ utils/config ] Imported diffuser.models.diffuser:GaussianDiffusion\n",
            "\n",
            "[utils/config ] Config: <class 'diffuser.models.diffuser.GaussianDiffusion'>\n",
            "    action_dim: 2\n",
            "    action_weight: 1\n",
            "    clip_denoised: True\n",
            "    horizon: 256\n",
            "    loss_discount: 1\n",
            "    loss_type: l2\n",
            "    loss_weights: None\n",
            "    n_timesteps: 256\n",
            "    observation_dim: 4\n",
            "    predict_epsilon: False\n",
            "\n",
            "[ utils/config ] Saved config to: saved/diffusion_config.pkl\n",
            "\n",
            "\n",
            "[utils/config ] Config: <class 'diffuser.utils.training.Trainer'>\n",
            "    bucket: None\n",
            "    ema_decay: 0.005\n",
            "    gradient_accumulate_every: 2\n",
            "    label_freq: 400\n",
            "    n_reference: 1\n",
            "    n_samples: 1\n",
            "    results_folder: saved/\n",
            "    sample_freq: 10\n",
            "    save_freq: 500\n",
            "    save_parallel: False\n",
            "    train_batch_size: 32\n",
            "    train_lr: 0.0002\n",
            "\n",
            "[ utils/config ] Saved config to: saved/trainer_config.pkl\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dataset_config = Config(\n",
        "    args.loader,\n",
        "    savepath=(args.savepath, \"dataset_config.pkl\"),\n",
        "    env=args.env_name,\n",
        "    horizon=args.horizon,\n",
        "    normalizer=args.normalizer,\n",
        "    preprocess_fns=args.preprocess_fns,\n",
        "    use_padding=args.use_padding,\n",
        "    max_path_length=args.max_path_length,\n",
        ")\n",
        "\n",
        "render_config = Config(\n",
        "    args.renderer,\n",
        "    savepath=(args.savepath, \"render_config.pkl\"),\n",
        "    env=args.env_name,\n",
        ")\n",
        "\n",
        "model_config = Config(\n",
        "    args.model,\n",
        "    savepath=(args.savepath, \"model_config.pkl\"),\n",
        "    horizon=args.horizon,\n",
        "    transition_dim=args.observation_dim + args.action_dim,\n",
        "    cond_dim=args.observation_dim,\n",
        "    dim_mults=args.dim_mults,\n",
        "    device=device,\n",
        ")\n",
        "diffusion_config = Config(\n",
        "    _class=\"models.diffuser.GaussianDiffusion\",\n",
        "    savepath=(args.savepath, \"diffusion_config.pkl\"),\n",
        "    horizon=args.horizon,\n",
        "    observation_dim=args.observation_dim,\n",
        "    action_dim=args.action_dim,\n",
        "    n_timesteps=args.n_timesteps,\n",
        "    loss_type=args.loss_type,\n",
        "    clip_denoised=args.clip_denoised,\n",
        "    predict_epsilon=args.predict_epsilon,\n",
        "    # loss weighting\n",
        "    action_weight=args.action_weight,\n",
        "    loss_weights=args.loss_weights,\n",
        "    loss_discount=args.loss_discount,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "trainer_config = Config(\n",
        "    Trainer,\n",
        "    savepath=(args.savepath, \"trainer_config.pkl\"),\n",
        "    train_batch_size=args.train_batch_size,\n",
        "    train_lr=args.train_lr,\n",
        "    gradient_accumulate_every=args.gradient_accumulate_every,\n",
        "    ema_decay=args.ema_decay,\n",
        "    sample_freq=args.sample_freq,\n",
        "    save_freq=args.save_freq,\n",
        "    label_freq=args.label_freq,\n",
        "    save_parallel=args.save_parallel,\n",
        "    results_folder=args.savepath,\n",
        "    bucket=args.bucket,\n",
        "    n_reference=args.n_reference,\n",
        "    n_samples=args.n_samples,\n",
        "    device=device,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yN1wOeVGTaft",
        "outputId": "1d0e9bfe-1bca-4718-a6e9-03954dd5dbc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name of the enviornment: PointMaze_Medium-v3\n",
            "Name of the enviornment: PointMaze_Medium-v3\n",
            "Dataset_name:pointmaze-umaze-v5\n",
            "dataset exists!\n",
            "10000\n",
            "[ datasets/buffer ] Finalized replay buffer | 1 episodes\n",
            "[ datasets/buffer ] Fields:\n",
            "    achieved_goal: (1, 10000, 2)\n",
            "    desired_goal: (1, 10000, 2)\n",
            "    observations: (1, 10000, 4)\n",
            "    observation: (1, 10000, 4)\n",
            "    next_observations: (1, 10000, 4)\n",
            "    actions: (1, 10000, 2)\n",
            "    terminals: (1, 10000, 1)\n",
            "    timeouts: (1, 10000, 1)\n",
            "[ datasets/buffer ] Fields:\n",
            "    achieved_goal: (1, 10000, 2)\n",
            "    desired_goal: (1, 10000, 2)\n",
            "    observations: (1, 10000, 4)\n",
            "    observation: (1, 10000, 4)\n",
            "    next_observations: (1, 10000, 4)\n",
            "    actions: (1, 10000, 2)\n",
            "    terminals: (1, 10000, 1)\n",
            "    timeouts: (1, 10000, 1)\n",
            "    normed_observations: (1, 10000, 4)\n",
            "    normed_actions: (1, 10000, 2)\n",
            "Name of the enviornment: PointMaze_Medium-v3\n",
            "[ models/temporal ] Channel dimensions: [(6, 32), (32, 128), (128, 256)]\n",
            "[(6, 32), (32, 128), (128, 256)]\n"
          ]
        }
      ],
      "source": [
        "# Load objects\n",
        "dataset = dataset_config()\n",
        "renderer = render_config()\n",
        "model = model_config()\n",
        "diffuser = diffusion_config(model)\n",
        "trainer = trainer_config(diffuser, dataset, renderer,device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcJZFQl9jXtn"
      },
      "source": [
        "## Forward pass is working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SnQpWsjKlRQ",
        "outputId": "6c4ce3f1-c507-42e2-e51b-485e83ed51ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ utils/arrays ] Total parameters: 3.68 M\n",
            "         downs.2.0.blocks.1.block.0.weight: 327.68 k | Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "         downs.2.1.blocks.0.block.0.weight: 327.68 k | Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "         downs.2.1.blocks.1.block.0.weight: 327.68 k | Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "         ups.0.0.blocks.0.block.0.weight: 327.68 k | Conv1d(512, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "         mid_block1.blocks.0.block.0.weight: 327.68 k | Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "         mid_block1.blocks.1.block.0.weight: 327.68 k | Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "         mid_block2.blocks.0.block.0.weight: 327.68 k | Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "         mid_block2.blocks.1.block.0.weight: 327.68 k | Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "         downs.2.0.blocks.0.block.0.weight: 163.84 k | Conv1d(128, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "         downs.1.0.blocks.1.block.0.weight: 81.92 k | Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "         ... and 138 others accounting for 814.92 k parameters\n",
            "Testing forward... ✓\n"
          ]
        }
      ],
      "source": [
        "report_parameters(model)\n",
        "\n",
        "print(\"Testing forward...\", end=\" \", flush=True)\n",
        "batch = batchify(dataset[0])\n",
        "loss, _ = diffuser.loss(*batch)\n",
        "loss.backward()\n",
        "print(\"✓\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2zObsfx8uFq"
      },
      "source": [
        "## Using the trainer requires taking care of the 'device' in the folders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRJlcy_WKXdq"
      },
      "source": [
        "# Training process inlcluding rendering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmagic_rabbit\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/magic-rabbit/tum-adlr-ss24-18/wandb/run-20240605_180041-ftswk4z2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/magic_rabbit/ADLR/runs/ftswk4z2' target=\"_blank\">Run_05_06_2024-18-00</a></strong> to <a href='https://wandb.ai/magic_rabbit/ADLR' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/magic_rabbit/ADLR' target=\"_blank\">https://wandb.ai/magic_rabbit/ADLR</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/magic_rabbit/ADLR/runs/ftswk4z2' target=\"_blank\">https://wandb.ai/magic_rabbit/ADLR/runs/ftswk4z2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "current_time = datetime.now().strftime(\"%d_%m_%Y-%H-%M\")\n",
        "\n",
        "if args.use_wandb:\n",
        "    run = wandb.init(\n",
        "        config=args,\n",
        "        project=args.wandb_project,\n",
        "        entity=args.wandb_entity,\n",
        "        name=f\"Run_{current_time}\",\n",
        "        group=\"Group-Name\",\n",
        "        job_type=\"training\",\n",
        "        reinit=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "id": "IaDt0qR3kczo",
        "outputId": "1ac1ee61-695b-4017-cd62-41bf7449b7b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 / 1 | saved/\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/magic-rabbit/tum-adlr-ss24-18/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv1d(input, weight, bias, self.stride,\n",
            "/home/magic-rabbit/tum-adlr-ss24-18/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Self_step:0\n",
            "Label_freq:400\n",
            "Label:0\n",
            "[ utils/training ] Saved model to saved/state_0.pt\n",
            "0:   0.2208 | a0_loss:   0.3511 | t:   0.6248\n",
            "100:   0.0393 | a0_loss:   0.2697 | t:   5.9963\n",
            "200:   0.0347 | a0_loss:   0.2909 | t:   6.1070\n",
            "300:   0.0283 | a0_loss:   0.2653 | t:   6.0193\n",
            "400:   0.0312 | a0_loss:   0.1926 | t:   6.4576\n"
          ]
        }
      ],
      "source": [
        "# n_epochs = int(args.n_train_steps // args.n_steps_per_epoch)\n",
        "n_epochs = 1\n",
        "diffuser.to(device)\n",
        "for i in range(n_epochs):\n",
        "    print(f\"Epoch {i} / {n_epochs} | {args.savepath}\")\n",
        "    trainer.train(n_train_steps=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = '/Users/magic-rabbit/Downloads/maze2d-large-dense-v1.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Replace 'your_file_path_here' with the actual path to the HDF5 file\u001b[39;00m\n\u001b[1;32m     13\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/magic-rabbit/Downloads/maze2d-large-dense-v1.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 14\u001b[0m observations, qpos \u001b[38;5;241m=\u001b[39m \u001b[43mextract_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Printing shapes of the arrays to confirm extraction\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObservations shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, observations\u001b[38;5;241m.\u001b[39mshape)\n",
            "Cell \u001b[0;32mIn[9], line 2\u001b[0m, in \u001b[0;36mextract_datasets\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_datasets\u001b[39m(file_path):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;66;03m# Extract observations dataset\u001b[39;00m\n\u001b[1;32m      4\u001b[0m         observations \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservations\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m# Extract infos/qpos dataset\u001b[39;00m\n",
            "File \u001b[0;32m~/tum-adlr-ss24-18/.venv/lib/python3.10/site-packages/h5py/_hl/files.py:562\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    553\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    554\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    555\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    556\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    557\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    558\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    559\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    560\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    561\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 562\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
            "File \u001b[0;32m~/tum-adlr-ss24-18/.venv/lib/python3.10/site-packages/h5py/_hl/files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
            "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = '/Users/magic-rabbit/Downloads/maze2d-large-dense-v1.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ],
      "source": [
        "def extract_datasets(file_path):\n",
        "    with h5py.File(file_path, \"r\") as f:\n",
        "        # Extract observations dataset\n",
        "        observations = np.array(f[\"observations\"])\n",
        "\n",
        "        # Extract infos/qpos dataset\n",
        "        qpos = np.array(f[\"infos/qpos\"])\n",
        "\n",
        "    return observations, qpos\n",
        "\n",
        "\n",
        "# Replace 'your_file_path_here' with the actual path to the HDF5 file\n",
        "file_path = \"/Users/magic-rabbit/Downloads/maze2d-large-dense-v1.hdf5\"\n",
        "observations, qpos = extract_datasets(file_path)\n",
        "\n",
        "# Printing shapes of the arrays to confirm extraction\n",
        "print(\"Observations shape:\", observations.shape)\n",
        "print(\"Qpos shape:\", qpos.shape)\n",
        "\n",
        "LARGE_MAZE = [\n",
        "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "    [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
        "    [1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
        "    [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "    [1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1],\n",
        "    [1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1],\n",
        "    [1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1],\n",
        "    [1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "]\n",
        "background_array = np.array(LARGE_MAZE)\n",
        "\n",
        "# Define the extent to center the plot at (0, 0)\n",
        "\n",
        "plt.clf()\n",
        "fig = plt.gcf()\n",
        "\n",
        "plt.imshow(\n",
        "    background_array,\n",
        "    cmap=plt.cm.binary,\n",
        "    # vmin=0,\n",
        "    # vmax=1,\n",
        ")\n",
        "\n",
        "path_length = len(observations)\n",
        "# observations = observations.reshape(len(observations), -1)\n",
        "colors = plt.cm.jet(np.linspace(0, 1, 100000))\n",
        "plt.plot(observations[:100000, 1], observations[:100000, 0], c=\"black\", zorder=10)\n",
        "plt.scatter(observations[:100000, 1], observations[:100000, 0], c=colors, zorder=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Medium\n",
        "\n",
        "# Replace 'your_file_path_here' with the actual path to the HDF5 file\n",
        "file_path = \"/Users/magic-rabbit/Downloads/maze2d-medium-dense-v1.hdf5\"\n",
        "observations, qpos = extract_datasets(file_path)\n",
        "\n",
        "# Printing shapes of the arrays to confirm extraction\n",
        "print(\"Observations shape:\", observations.shape)\n",
        "print(\"Qpos shape:\", qpos.shape)\n",
        "\n",
        "MEDIUM_MAZE = [\n",
        "    [1, 1, 1, 1, 1, 1, 1, 1],\n",
        "    [1, 0, 0, 1, 1, 0, 0, 1],\n",
        "    [1, 0, 0, 1, 0, 0, 0, 1],\n",
        "    [1, 1, 0, 0, 0, 1, 1, 1],\n",
        "    [1, 0, 0, 1, 0, 0, 0, 1],\n",
        "    [1, 0, 1, 0, 0, 1, 0, 1],\n",
        "    [1, 0, 0, 0, 1, 0, 0, 1],\n",
        "    [1, 1, 1, 1, 1, 1, 1, 1],\n",
        "]\n",
        "background_array = np.array(MEDIUM_MAZE)\n",
        "\n",
        "\n",
        "plt.clf()\n",
        "fig = plt.gcf()\n",
        "\n",
        "plt.imshow(\n",
        "    background_array,\n",
        "    cmap=plt.cm.binary,\n",
        "    # vmin=0,\n",
        "    # vmax=1,\n",
        ")\n",
        "\n",
        "path_length = len(observations)\n",
        "# observations = observations.reshape(len(observations), -1)\n",
        "colors = plt.cm.jet(np.linspace(0, 1, 100000))\n",
        "plt.plot(observations[:100000, 1], observations[:100000, 0], c=\"black\", zorder=10)\n",
        "plt.scatter(observations[:100000, 1], observations[:100000, 0], c=colors, zorder=20)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "yP4EZWUtjLEW",
        "Xh7nFzc5jxhB"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
